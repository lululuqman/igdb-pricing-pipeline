import psycopg2
import os
from dotenv import load_dotenv
from io import StringIO
import pandas as pd

load_dotenv()

# Database credentials
DB_NAME = os.getenv("POSTGRES_DB")
DB_USER = os.getenv("POSTGRES_USER")
DB_PASS = os.getenv("POSTGRES_PASSWORD")
DB_HOST = os.getenv("POSTGRES_HOST")
DB_PORT = os.getenv("POSTGRES_PORT")

STAGING_TABLE_NAME = "game_data_staging"
FINAL_TABLE_NAME = "game_data"
CSV_PATH = "data/clean_games_data.csv"

def get_db_connection():
    """Establishes and returns a connection to the PostgreSQL database."""
    print("-> Connecting to PostgreSQL...")
    conn = psycopg2.connect(
        dbname=DB_NAME,
        user=DB_USER,
        password=DB_PASS,
        host=DB_HOST,
        port=DB_PORT
    )
    print("-> Connection successful.")
    return conn

def create_table_if_not_exists(conn):
    """Creates the Final Table and the Staging Table if they do not exist."""
    cursor = conn.cursor()

    # Schema definition - used for BOTH tables
    schema = """
        id INT PRIMARY KEY,
        game_name VARCHAR(255) NOT NULL,
        release_date DATE,
        genres TEXT,
        avg_rating DECIMAL(4, 2),
        base_price_usd DECIMAL(6, 2),
        base_price_myr DECIMAL(6, 2),
        load_timestamp TIMESTAMP WITHOUT TIME ZONE DEFAULT NOW()
    """
    
    # 1. Create the FINAL table
    final_table_command = f"""
    CREATE TABLE IF NOT EXISTS {FINAL_TABLE_NAME} ({schema});
    """
    cursor.execute(final_table_command)

    # 2. Create the STAGING table (This one does NOT need a primary key, 
    # but for simplicity, we'll keep the schema the same)
    staging_table_command = f"""
    CREATE TABLE IF NOT EXISTS {STAGING_TABLE_NAME} ({schema});
    """
    cursor.execute(staging_table_command)

    conn.commit()
    print(f"-> Tables '{FINAL_TABLE_NAME}' and '{STAGING_TABLE_NAME}' created or already exist.")
    cursor.close()

def load_to_staging(conn):
    """
    Loads CSV data into the temporary staging table (game_data_staging)
    using the highly efficient COPY command.
    """
    # NOTE: STAGING_TABLE_NAME must be defined globally at the top of src/load.py
    print(f"-> Starting bulk load into {STAGING_TABLE_NAME}...")
    cursor = conn.cursor()
    
    # Read the CSV generated by the transformation script
    df = pd.read_csv(CSV_PATH)

    # Use StringIO to create an in-memory file object for the COPY command
    csv_buffer = StringIO()
    # Ensure index=False to prevent an unwanted index column in the CSV
    df.to_csv(csv_buffer, index=False, header=True) 
    csv_buffer.seek(0)
    
    # 1. Define the columns present in your CSV file
    csv_columns = [
        'id', 'game_name', 'release_date', 'genres', 
        'avg_rating', 'base_price_usd', 'base_price_myr'
    ]
    columns_str = ', '.join(csv_columns)
    
    # 2. Update the COPY command: Target STAGING_TABLE_NAME and list columns
    copy_sql = f"""
        COPY {STAGING_TABLE_NAME} ({columns_str}) FROM STDIN WITH CSV HEADER DELIMITER AS ','
    """
    
    # Execute the COPY command
    cursor.copy_expert(sql=copy_sql, file=csv_buffer)
    conn.commit()
    
    print(f"✅ Successfully loaded {len(df)} rows into {STAGING_TABLE_NAME}.")
    cursor.close()

def upsert_data(conn):
    """
    Merges data from the staging table into the final table.
    If a row's ID exists, it updates the row; otherwise, it inserts the row.
    """
    print(f"-> Starting UPSERT (Merge) into {FINAL_TABLE_NAME}...")
    cursor = conn.cursor()

    # 1. TRUNCATE the staging table before we load fresh data into it.
    truncate_staging_command = f"TRUNCATE TABLE {STAGING_TABLE_NAME};"
    cursor.execute(truncate_staging_command)
    print(f"-> Staging table '{STAGING_TABLE_NAME}' cleared.")

    # 2. The UPSERT/Merge SQL command
    # It attempts to insert ALL rows from the staging table into the final table.
    # If a conflict occurs on the 'id' (the Primary Key), it updates the existing row.
    upsert_command = f"""
    INSERT INTO {FINAL_TABLE_NAME} (
        id, game_name, release_date, genres, avg_rating, base_price_usd, base_price_myr, load_timestamp
    )
    SELECT 
        id, game_name, release_date, genres, avg_rating, base_price_usd, base_price_myr, NOW()
    FROM {STAGING_TABLE_NAME}
    ON CONFLICT (id) DO UPDATE SET
        game_name = EXCLUDED.game_name,
        release_date = EXCLUDED.release_date,
        genres = EXCLUDED.genres,
        avg_rating = EXCLUDED.avg_rating,
        base_price_usd = EXCLUDED.base_price_usd,
        base_price_myr = EXCLUDED.base_price_myr,
        load_timestamp = NOW();
    """
    cursor.execute(upsert_command)
    conn.commit()
    print(f"✅ UPSERT Complete. Data merged into {FINAL_TABLE_NAME}.")
    cursor.close()

def run_load_pipeline():
    """Orchestrates the connection, table creation, staging load, and final UPSERT."""
    conn = None
    try:
        conn = get_db_connection()
        
        # 1. Ensure tables exist
        create_table_if_not_exists(conn)
        
        # 2. Load fresh data into the staging table (and truncate staging first)
        load_to_staging(conn) 
        
        # 3. Merge data from staging to the final table (The UPSERT)
        upsert_data(conn)
        
    except psycopg2.Error as e:
        print(f"❌ Database Error: {e}")
        if conn:
            conn.rollback() # Rollback the transaction on error
    except Exception as e:
        print(f"❌ An unexpected error occurred: {e}")
    finally:
        if conn:
            conn.close()
            print("-> Database connection closed.")
        
if __name__ == "__main__":
    run_load_pipeline()